# SQBCP â€“ Sequence & Structure Based Cofactor Binding Predictor

**Dual-branch** prediktor vazby NAD kofaktoru, vyuÅ¾Ã­vajÃ­cÃ­ ESM-2 embeddingy, grafovou neuronovou sÃ­Å¥ (GAT/GCN) pro strukturnÃ­ data a 1D-CNN+Attention vÄ›tev pro sekvence bez struktury.

> **KlÃ­ÄovÃ¡ vlastnost:** Model se uÄÃ­ jak z PDB struktur (stovky), tak z anotovanÃ½ch sekvencÃ­ bez struktury (tisÃ­ce z UniProt), ÄÃ­mÅ¾ vyuÅ¾Ã­vÃ¡ mnohem vÃ­ce dat.

---

## Architektura & logika

### Dual-branch architektura

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚           VSTUPNÃ DATA                  â”‚
                â”‚                                         â”‚
                â”‚  PDB struktury        Sekvence (UniProt)â”‚
                â”‚  (~500 s NAD)         (~10 000 s anotacÃ­)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                     â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  GNN Branch           â”‚  â”‚ Sequence Branch      â”‚
            â”‚  (binding_site_graph) â”‚  â”‚ (sequence_dataset)   â”‚
            â”‚                       â”‚  â”‚                      â”‚
            â”‚  PDB â†’ Binding Site   â”‚  â”‚ Sekvence â†’ ESM-2     â”‚
            â”‚  â†’ kontaktnÃ­ mapa     â”‚  â”‚ embeddingy           â”‚
            â”‚  â†’ PyG graf           â”‚  â”‚ â†’ 1D-CNN (local      â”‚
            â”‚  â†’ GAT/GCN vrstvy    â”‚  â”‚   motifs)            â”‚
            â”‚  â†’ Attn pooling       â”‚  â”‚ â†’ Self-Attention     â”‚
            â”‚                       â”‚  â”‚ â†’ Learned pooling    â”‚
            â”‚  [B, hidden_dim]      â”‚  â”‚ [B, hidden_dim]      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Shared Classifier   â”‚
                         â”‚  (sdÃ­lenÃ½ MLP)       â”‚
                         â”‚  â†’ 2 tÅ™Ã­dy           â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                             P(binds NAD)
```

### Detail GNN Branch (strukturnÃ­ data)

```
PDB soubor
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Binding Site Extractorâ”‚  (Binding_site_ex.py)
â”‚  - parsuje PDB strukturu â”‚
â”‚  - najde ligand (NAD)    â”‚
â”‚  - identifikuje residues â”‚
â”‚    do 6 Ã… od ligandu     â”‚
â”‚  - vytvoÅ™Ã­ kontaktnÃ­ mapuâ”‚
â”‚    (CÎ±-CÎ± < 8 Ã…)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. ESM-2 + Features     â”‚  (esm2_feature_ex.py + additional_features.py)
â”‚  ESM-2 embeddingy [1280] â”‚
â”‚  + BLOSUM62 [20]         â”‚
â”‚  + Physicochemical [7]   â”‚
â”‚  + Position [3]          â”‚
â”‚  = 1310D na uzel         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. GNN (GAT/GCN)        â”‚  (dual_predictor.py â†’ GNNBranch)
â”‚  - 3Ã— GAT vrstvy        â”‚
â”‚  - Attention pooling     â”‚
â”‚  â†’ graph embedding [256] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detail Sequence Branch (sekvence bez struktury)

```
Sekvence (string)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ESM-2 Embeddings     â”‚  (esm2_feature_ex.py)
â”‚  â†’ per-residue [L, 1280] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Sequence Branch      â”‚  (dual_predictor.py â†’ SequenceBranch)
â”‚  - Input projection      â”‚
â”‚  - 3Ã— 1D-CNN (lokÃ¡lnÃ­    â”‚
â”‚    motivy, kernel=5)     â”‚
â”‚  - Self-Attention        â”‚
â”‚  - Learned pooling       â”‚
â”‚  â†’ seq embedding [256]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### TrÃ©novacÃ­ reÅ¾imy

| ReÅ¾im | Vstup | VÄ›tev | Kdy pouÅ¾Ã­t |
|-------|-------|-------|------------|
| `sequence` | Sekvence (ESM emb.) | Seq branch + classifier | Sekvence bez struktury |
| `structure` | PyG graf | GNN branch + classifier | PDB data se strukturou |
| `both` | Oboje | ObÄ› + fusion + consistency | PDB data (obÄ› vÄ›tve) |

### ProÄ tento pÅ™Ã­stup?

1. **ESM-2 embeddingy** â€“ protein language model trÃ©novanÃ½ na milionech sekvencÃ­ zachycuje evoluÄnÃ­ a strukturnÃ­ informaci bez potÅ™eby MSA
2. **GrafovÃ¡ reprezentace** â€“ binding site je pÅ™irozenÄ› graf (residues = uzly, prostorovÃ© kontakty = hrany), GNN propaguje informaci po struktuÅ™e
3. **GAT (Graph Attention)** â€“ uÄÃ­ se, kterÃ© kontakty jsou dÅ¯leÅ¾itÄ›jÅ¡Ã­ pro predikci, vhodnÃ© pro malÃ© grafy (15â€“30 uzlÅ¯)
4. **Multi-head attention pooling** â€“ agreguje uzlovÃ© embeddingy do jednoho grafovÃ©ho vektoru s nauÄenÃ½mi vahami
5. **Dual-branch** â€“ vyuÅ¾Ã­vÃ¡ mnohem vÃ­ce sekvenÄnÃ­ch dat (tisÃ­ce z UniProt) vedle stovek PDB struktur
6. **SdÃ­lenÃ½ classifier** â€“ obÄ› vÄ›tve sdÃ­lejÃ­ klasifikaÄnÃ­ hlavu â†’ sekvenÄnÃ­ data pomÃ¡hajÃ­ i GNN vÄ›tvi
7. **Consistency loss** â€“ na PDB datech penalizuje rozdÃ­l mezi GNN a Seq embeddingy â†’ Seq branch se uÄÃ­ aproximovat strukturnÃ­ informaci

---

## Instalace

```bash
# 1. VytvoÅ™it conda environment
conda env create -f environment.yml

# 2. Aktivovat
conda activate sqbcp
```

### GPU verze

V `environment.yml` zmÄ›nit:
```yaml
# smazat:
- cpuonly
# pÅ™idat:
- pytorch-cuda=12.1    # nebo vaÅ¡e verze CUDA
```

---

## PÅ™Ã­prava dat

### VstupnÃ­ data

- PDB soubory s navÃ¡zanÃ½m ligandem (NAD, ATP, FAD, ...)
- UmÃ­stit do sloÅ¾ky, napÅ™. `./pdb_files/`

### Struktura PDB souboru

- MusÃ­ obsahovat protein chain s â‰¥10 residues
- MusÃ­ obsahovat ligand jako HETATM zÃ¡znam s danÃ½m jmÃ©nem (napÅ™. `NAD`)

---

## PouÅ¾itÃ­ â€“ krok po kroku

### Krok 1: Extrakce binding sites z PDB

```python
from Binding_site_ex import BindingSiteExtractor
import glob

extractor = BindingSiteExtractor(distance_threshold=6.0)

pdb_files = glob.glob('./pdb_files/*.pdb')

binding_sites = []
for pdb_file in pdb_files:
    try:
        bs_info = extractor.extract_binding_site(pdb_file, ligand_name='NAD')
        binding_sites.append(bs_info)
        print(f"{pdb_file}: {bs_info['n_binding_site']} residues")
    except Exception as e:
        print(f"Error {pdb_file}: {e}")

print(f"Celkem: {len(binding_sites)} struktur")
```

**Parametry:**
- `distance_threshold` â€“ maximÃ¡lnÃ­ vzdÃ¡lenost atomu residue od ligandu (v Ã…), default 6.0
- `ligand_name` â€“ tÅ™Ã­pÃ­smennÃ½ kÃ³d ligandu v PDB (NAD, ATP, FAD, ...)

### Krok 2: Extrakce ESM-2 embeddingÅ¯

```python
from esm2_feature_ex import ESMFeatureExtractor

esm_extractor = ESMFeatureExtractor(
    model_name="facebook/esm2_t33_650M_UR50D"
)

for bs_info in binding_sites:
    bs_embeddings = esm_extractor.extract_binding_site_embeddings(
        bs_info['full_sequence'],
        bs_info['binding_site_indices']
    )
    bs_info['esm_embeddings'] = bs_embeddings
    print(f"Embeddings shape: {bs_embeddings.shape}")
```

**DostupnÃ© modely ESM-2:**

| Model | Parametry | Embedding dim | PamÄ›Å¥ |
|-------|-----------|---------------|-------|
| `esm2_t30_150M_UR50D` | 150M | 640 | ~1 GB |
| `esm2_t33_650M_UR50D` | 650M | 1280 | ~3 GB |
| `esm2_t36_3B_UR50D` | 3B | 2560 | ~12 GB |

> **Pozor:** PÅ™i zmÄ›nÄ› modelu se zmÄ›nÃ­ `node_dim` v prediktoru!  
> 640 + 30 = 670 (pro 150M), 1280 + 30 = 1310 (pro 650M), 2560 + 30 = 2590 (pro 3B)

### Krok 3: SestavenÃ­ grafovÃ©ho datasetu

```python
from binding_site_graph import BindingSiteGraphDataset

dataset = BindingSiteGraphDataset(
    binding_sites,
    feature_config={
        'use_esm': True,       # ESM-2 embeddingy (1280D)
        'use_blosum': True,    # BLOSUM62 encoding (20D)
        'use_physchem': True,  # Physicochemical (7D)
        'use_position': True   # RelativnÃ­ pozice (3D)
    }
)

print(f"GrafÅ¯: {len(dataset)}")
print(f"Node features dim: {dataset[0].x.shape[1]}")
```

### Krok 4a: TrÃ©nink (jen PDB struktury â€“ pÅ¯vodnÃ­ pipeline)

```python
import torch
from binding_site_predictor import BindingSiteNADPredictor
from train import Trainer
from torch_geometric.loader import DataLoader
from sklearn.model_selection import train_test_split

model = BindingSiteNADPredictor(node_dim=1310, use_gat=True)
train_graphs, val_graphs = train_test_split(dataset.graphs, test_size=0.2)
train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)
val_loader = DataLoader(val_graphs, batch_size=32)

trainer = Trainer(model, train_loader, val_loader, device='cpu')
trainer.train(num_epochs=100)
```

### Krok 4b: Dual trÃ©nink (PDB + sekvence â€“ doporuÄeno) ğŸ†•

VyuÅ¾Ã­vÃ¡ mnohem vÃ­ce dat â€“ sekvence z UniProt bez nutnosti 3D struktury:

```python
import torch
from dual_predictor import DualBranchPredictor
from dual_train import DualTrainer
from sequence_dataset import SequenceDataset, collate_sequences, load_sequences_from_csv
from torch.utils.data import DataLoader
from torch_geometric.loader import DataLoader as PyGDataLoader
from sklearn.model_selection import train_test_split

# 1. StrukturnÃ­ data (PDB) â€“ stÃ¡vajÃ­cÃ­ pipeline
train_graphs, val_graphs = train_test_split(dataset.graphs, test_size=0.2)
graph_train_loader = PyGDataLoader(train_graphs, batch_size=32, shuffle=True)
graph_val_loader = PyGDataLoader(val_graphs, batch_size=32)

# 2. SekvenÄnÃ­ data (UniProt) â€“ NOVÃ zdroj dat
sequences, labels = load_sequences_from_csv('data/nad_sequences.csv')
seq_dataset = SequenceDataset(sequences, labels, esm_extractor=esm)
seq_train, seq_val = train_test_split(list(range(len(seq_dataset))), test_size=0.2)
seq_train_loader = DataLoader(
    torch.utils.data.Subset(seq_dataset, seq_train),
    batch_size=16, shuffle=True, collate_fn=collate_sequences
)
seq_val_loader = DataLoader(
    torch.utils.data.Subset(seq_dataset, seq_val),
    batch_size=16, collate_fn=collate_sequences
)

# 3. Dual-branch model
model = DualBranchPredictor(
    esm_dim=1280, node_dim=1310, hidden_dim=256,
    num_gnn_layers=3, use_gat=True
)

# 4. Dual trainer
trainer = DualTrainer(
    model=model,
    graph_train_loader=graph_train_loader,
    graph_val_loader=graph_val_loader,
    seq_train_loader=seq_train_loader,
    seq_val_loader=seq_val_loader,
    device='cuda' if torch.cuda.is_available() else 'cpu',
    consistency_weight=0.3,
)
trainer.train(num_epochs=100)
```

NejlepÅ¡Ã­ model se automaticky uloÅ¾Ã­ jako `best_dual_model.pth`.

**VÃ½stup trÃ©ninku:**
```
Epoch 1/100
  Train - Loss: 0.6932, Acc: 0.5200
  Val   - Loss: 0.6815, Acc: 0.5800, AUC: 0.6120
  â†’ New best AUC: 0.6120
...
```

---

## Predikce

### A) Ze znÃ¡mÃ© struktury (PDB soubor)

```python
from Binding_site_ex import BindingSiteExtractor
from esm2_feature_ex import ESMFeatureExtractor
from additional_features import create_node_features
from binding_site_predictor import BindingSiteNADPredictor
from binding_site_graph import BindingSiteGraphDataset
import torch
import torch.nn.functional as F

# 1. NaÄÃ­st model
model = BindingSiteNADPredictor(node_dim=1310, use_gat=True)
model.load_state_dict(torch.load('best_model.pth'))
model.eval()

# 2. Extrahovat binding site
extractor = BindingSiteExtractor(distance_threshold=6.0)
bs_info = extractor.extract_binding_site('query.pdb', ligand_name='NAD')

# 3. ESM embeddingy
esm = ESMFeatureExtractor()
bs_info['esm_embeddings'] = esm.extract_binding_site_embeddings(
    bs_info['full_sequence'], bs_info['binding_site_indices']
)

# 4. Sestavit graf
dataset = BindingSiteGraphDataset([bs_info])
graph = dataset[0]

# 5. Predikce
with torch.no_grad():
    logits = model(graph)
    prob = F.softmax(logits, dim=1)[0, 1].item()

print(f"P(binds NAD) = {prob:.4f}")
```

### B) Jen ze sekvence â€“ Dual model (doporuÄeno) ğŸ†•

```python
from dual_predictor import DualBranchPredictor
from esm2_feature_ex import ESMFeatureExtractor
import torch
import torch.nn.functional as F

# NaÄÃ­st dual model
model = DualBranchPredictor(esm_dim=1280, node_dim=1310, use_gat=True)
model.load_state_dict(torch.load('best_dual_model.pth'))
model.eval()

# ESM embeddings
esm = ESMFeatureExtractor()
sequence = "MKVLITGAGSGIGKAIA..."
emb = esm.extract_embeddings(sequence)  # [L, 1280]

# Predikce (sequence-only mode)
with torch.no_grad():
    esm_tensor = torch.FloatTensor(emb).unsqueeze(0)  # [1, L, 1280]
    logits, _ = model(mode='sequence', esm_embeddings=esm_tensor)
    prob = F.softmax(logits, dim=1)[0, 1].item()

print(f"P(binds NAD) = {prob:.4f}")
```

> **VÃ½hoda dual modelu:** NevyÅ¾aduje kontaktnÃ­ mapu ani 3D strukturu. Seq branch se uÄil na tisÃ­cÃ­ch sekvencÃ­ â†’ pÅ™esnÄ›jÅ¡Ã­ neÅ¾ starÃ½ `seq_only_predictor.py`.

### C) Jen ze sekvence â€“ starÃ½ pÅ™Ã­stup (vyÅ¾aduje contact predictor)

```python
from seq_only_predictor import SequenceOnlyPredictor

predictor = SequenceOnlyPredictor(model, esm_extractor, contact_predictor)

sequence = "MKVLITGAGSGIGKAIA..."
prob = predictor.predict(sequence)
print(f"P(binds NAD) = {prob:.4f}")
```

> **PoznÃ¡mka:** StarÅ¡Ã­ pÅ™Ã­stup â€“ vyÅ¾aduje contact predictor pro odhad kontaktnÃ­ mapy.

---

## Struktura projektu

```
â”œâ”€â”€ environment.yml           # Conda environment
â”œâ”€â”€ README.md                 # Tento soubor
â”‚
â”‚â”€â”€ # Data pipeline
â”œâ”€â”€ Binding_site_ex.py        # Extrakce binding site z PDB
â”œâ”€â”€ esm2_feature_ex.py        # ESM-2 protein embeddingy
â”œâ”€â”€ additional_features.py    # BLOSUM, physicochemical, pozice
â”œâ”€â”€ binding_site_graph.py     # Konstrukce PyG grafÅ¯
â”œâ”€â”€ sequence_dataset.py       # ğŸ†• Dataset pro sekvence bez struktury
â”‚
â”‚â”€â”€ # Modely
â”œâ”€â”€ binding_site_predictor.py # GNN-only model (GAT/GCN)
â”œâ”€â”€ dual_predictor.py         # ğŸ†• Dual-branch model (GNN + Seq)
â”œâ”€â”€ seq_only_predictor.py     # StarÅ¡Ã­ seq-only inference wrapper
â”‚
â”‚â”€â”€ # TrÃ©nink
â”œâ”€â”€ train.py                  # TrÃ©ninkovÃ¡ smyÄka (GNN-only)
â”œâ”€â”€ dual_train.py             # ğŸ†• Dual training (PDB + sekvence)
â”‚
â”œâ”€â”€ *.pdb                     # PDB vstupnÃ­ soubory
â”œâ”€â”€ best_model.pth            # UloÅ¾enÃ½ GNN-only model
â””â”€â”€ best_dual_model.pth       # ğŸ†• UloÅ¾enÃ½ dual model
```

## DÅ¯leÅ¾itÃ© poznÃ¡mky

1. **NegativnÃ­ vzorky** â€“ v aktuÃ¡lnÃ­ verzi jsou vÅ¡echny PDB struktury pozitivnÃ­ (obsahujÃ­ NAD). Pro trÃ©nink je nutnÃ© pÅ™idat negativnÃ­ vzorky (proteiny, kterÃ© NAD nevÃ¡Å¾ou), jinak model nebude schopen rozliÅ¡ovat.

2. **BLOSUM62** â€“ metoda `_load_blosum62()` v `additional_features.py` je zatÃ­m placeholder. Pro plnou funkÄnost je potÅ™eba doplnit skuteÄnou BLOSUM62 matici.

3. **Batch size** â€“ pro malÃ© datasety (<100 grafÅ¯) sniÅ¾te `batch_size` na 8â€“16.

4. **PÅ™etrÃ©novÃ¡nÃ­** â€“ model mÃ¡ ~500k parametrÅ¯. PÅ™i malÃ©m datasetu zvaÅ¾te:
   - ZvÃ½Å¡it `dropout` (0.5 â†’ 0.7)
   - SnÃ­Å¾it `hidden_dim` (256 â†’ 128)
   - PouÅ¾Ã­t mÃ©nÄ› GNN vrstev (3 â†’ 2)

---

## Citace & reference

- **ESM-2**: Lin et al. (2023) "Evolutionary-scale prediction of atomic-level protein structure with a language model." *Science*
- **Kyte-Doolittle**: Kyte & Doolittle (1982) "A simple method for displaying the hydropathic character of a protein." *J Mol Biol* 157:105-132
- **BLOSUM62**: Henikoff & Henikoff (1992) "Amino acid substitution matrices from protein blocks." *PNAS* 89:10915-10919
- **Chou-Fasman**: Chou & Fasman (1978) "Prediction of the secondary structure of proteins from their amino acid sequence." *Adv Enzymol* 47:45-148
- **PyTorch Geometric**: Fey & Lenssen (2019) "Fast Graph Representation Learning with PyTorch Geometric." *ICLR Workshop*
